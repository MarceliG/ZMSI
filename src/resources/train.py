import pandas as pd
import torch
from datasets import Dataset
from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments

from src.config import FilePath
from src.logs import logger
from src.resources import load_dataset_from_path


def preprocess_data(dataset: Dataset) -> Dataset:
    """Tokenizes and preprocesses the dataset."""
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokenized_data = tokenizer(dataset["text"], truncation=True, padding="max_length", max_length=128)
    # Konwertowanie tokenizowanych danych do ramki danych
    tokenized_df = pd.DataFrame(
        {
            "input_ids": tokenized_data["input_ids"],
            "attention_mask": tokenized_data["attention_mask"],
            "labels": dataset["rating"],
        }
    )

    tokenized_df["labels"] = tokenized_df["labels"].astype(int) - 1

    return Dataset.from_pandas(tokenized_df)


def train_model(dataset_path: str) -> None:
    """Train the model using the dataset at the given path."""
    logger.info("Start training model")
    # Load the dataset
    dataset = load_dataset_from_path(dataset_path)
    if dataset is None:
        msg = "Dataset not found"
        raise ValueError(msg)
    tokenized_datasets = preprocess_data(dataset["train"])  # @TODO czy na pewno tak ma byÄ‡?

    # Initialize the model
    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=5)

    training_args = TrainingArguments(
        output_dir=FilePath().results,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        gradient_accumulation_steps=2,
        fp16=torch.cuda.is_available(),
    )

    # Train the model
    trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_datasets)
    trainer.train()

    model.save_pretrained(FilePath().models)

    logger.info("Model training completed")
